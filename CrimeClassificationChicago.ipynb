{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to execute python and spark commands and libraries\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"clipper-pyspark\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries to perform visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "np.random.seed(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Structure\n",
      "----------------------------------\n",
      "root\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n",
      "None\n",
      " \n",
      "Dataframe preview\n",
      "+-------------------+--------------------+\n",
      "|       Primary Type|         Description|\n",
      "+-------------------+--------------------+\n",
      "| deceptive practice|financial identit...|\n",
      "|crim sexual assault|      non-aggravated|\n",
      "|           burglary|      unlawful entry|\n",
      "|              theft|           over $500|\n",
      "|crim sexual assault|      non-aggravated|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      " \n",
      "----------------------------------\n",
      "Total number of rows 7032560\n"
     ]
    }
   ],
   "source": [
    "#Read the data into spark datafrome\n",
    "from pyspark.sql.functions import col, lower\n",
    "df = spark.read.csv('/Users/manje/Documents/MANJEERA/SEM 2/BIG DATA ANALYTICS/TECHNIAL PROJECT/output_file_final.csv', inferSchema=True, header=True)\n",
    "\n",
    "data = df.select(lower(col('Primary Type')),lower(col('Description')))\\\n",
    "        .withColumnRenamed('lower(Primary Type)','Primary Type')\\\n",
    "        .withColumnRenamed('lower(Description)', 'Description')\n",
    "data.cache()\n",
    "print('Dataframe Structure')\n",
    "print('----------------------------------')\n",
    "print(data.printSchema())\n",
    "print(' ')\n",
    "print('Dataframe preview')\n",
    "print(data.show(5))\n",
    "print(' ')\n",
    "print('----------------------------------')\n",
    "print('Total number of rows', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique value of Primary Type: 35\n",
      " \n",
      "Top 10 Crime Primary Type\n",
      "+-------------------+----------+\n",
      "|       Primary Type|totalValue|\n",
      "+-------------------+----------+\n",
      "|              theft|   1486432|\n",
      "|            battery|   1285561|\n",
      "|    criminal damage|    800484|\n",
      "|          narcotics|    726658|\n",
      "|            assault|    440824|\n",
      "|      other offense|    436869|\n",
      "|           burglary|    398672|\n",
      "|motor vehicle theft|    324105|\n",
      "| deceptive practice|    284366|\n",
      "|            robbery|    264485|\n",
      "+-------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      " \n",
      " \n",
      "Total number of unique value of Description: 381\n",
      " \n",
      "Top 10 Crime Description\n",
      "+--------------------+----------+\n",
      "|         Description|totalValue|\n",
      "+--------------------+----------+\n",
      "|              simple|    829104|\n",
      "|      $500 and under|    572188|\n",
      "|domestic battery ...|    542802|\n",
      "|          to vehicle|    388870|\n",
      "|         to property|    368875|\n",
      "|           over $500|    364510|\n",
      "|poss: cannabis 30...|    278021|\n",
      "|      forcible entry|    269153|\n",
      "|          automobile|    254900|\n",
      "|       from building|    238499|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Location Description`' given input columns: [Primary Type, Description];;\\n'Project ['Location Description]\\n+- Project [Primary Type#278, lower(Description)#275 AS Description#281]\\n   +- Project [lower(Primary Type)#274 AS Primary Type#278, lower(Description)#275]\\n      +- Project [lower(Primary Type#231) AS lower(Primary Type)#274, lower(Description#232) AS lower(Description)#275]\\n         +- Relation[ID#226,Case Number#227,Date#228,Block#229,IUCR#230,Primary Type#231,Description#232,Location Description#233,Arrest#234,Domestic#235,Beat#236,District#237,Ward#238,Community Area#239,FBI Code#240,X Coordinate#241,Y Coordinate#242,Year#243,Updated On#244,Latitude#245,Longitude#246,Location#247,Crime_Date#248,Crime_Time#249] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o168.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Location Description`' given input columns: [Primary Type, Description];;\n'Project ['Location Description]\n+- Project [Primary Type#278, lower(Description)#275 AS Description#281]\n   +- Project [lower(Primary Type)#274 AS Primary Type#278, lower(Description)#275]\n      +- Project [lower(Primary Type#231) AS lower(Primary Type)#274, lower(Description#232) AS lower(Description)#275]\n         +- Relation[ID#226,Case Number#227,Date#228,Block#229,IUCR#230,Primary Type#231,Description#232,Location Description#233,Arrest#234,Domestic#235,Beat#236,District#237,Ward#238,Community Area#239,FBI Code#240,X Coordinate#241,Y Coordinate#242,Year#243,Updated On#244,Latitude#245,Longitude#246,Location#247,Crime_Date#248,Crime_Time#249] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0f18555387e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtop_n_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtop_n_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Location Description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0f18555387e0>\u001b[0m in \u001b[0;36mtop_n_list\u001b[1;34m(df, var, N)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mdetermine\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mN\u001b[0m \u001b[0mnumbers\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     '''\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total number of unique value of\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Top'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'Crime'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \"\"\"\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`Location Description`' given input columns: [Primary Type, Description];;\\n'Project ['Location Description]\\n+- Project [Primary Type#278, lower(Description)#275 AS Description#281]\\n   +- Project [lower(Primary Type)#274 AS Primary Type#278, lower(Description)#275]\\n      +- Project [lower(Primary Type#231) AS lower(Primary Type)#274, lower(Description#232) AS lower(Description)#275]\\n         +- Relation[ID#226,Case Number#227,Date#228,Block#229,IUCR#230,Primary Type#231,Description#232,Location Description#233,Arrest#234,Domestic#235,Beat#236,District#237,Ward#238,Community Area#239,FBI Code#240,X Coordinate#241,Y Coordinate#242,Year#243,Updated On#244,Latitude#245,Longitude#246,Location#247,Crime_Date#248,Crime_Time#249] csv\\n\""
     ]
    }
   ],
   "source": [
    "#Defining the top categories of crime with the highest total count\n",
    "def top_n_list(df,var, N):\n",
    "    '''\n",
    "    This function determine the top N numbers of the list \n",
    "    '''\n",
    "    print(\"Total number of unique value of\"+' '+var+''+':'+' '+str(df.select(var).distinct().count()))\n",
    "    print(' ')\n",
    "    print('Top'+' '+str(N)+' '+'Crime'+' '+var)\n",
    "    df.groupBy(var).count().withColumnRenamed('count','totalValue')\\\n",
    "    .orderBy(col('totalValue').desc()).show(N)\n",
    "    \n",
    "    \n",
    "top_n_list(data, 'Primary Type',10)\n",
    "print(' ')\n",
    "print(' ')\n",
    "top_n_list(data,'Description',10)\n",
    "top_n_list(data,'Location Description',10)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the distinct crime count based on the type of crime\n",
    "data.select('Primary Type').distinct().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are splitting the input data into Training set and Test set randomly in order to train and test the model respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 4924319\n",
      "Test Dataset Count: 2108241\n"
     ]
    }
   ],
   "source": [
    "training, test = data.randomSplit([0.7,0.3], seed=60)\n",
    "#trainingSet.cache()\n",
    "print(\"Training Dataset Count:\", training.count())\n",
    "print(\"Test Dataset Count:\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is to import the various text classification libraries and to create a pipeline for the features extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, HashingTF, IDF, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes \n",
    "\n",
    "#Defining the tokenizer using regextokenizer function\n",
    "regex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n",
    "                  .setInputCol(\"Description\")\\\n",
    "                  .setOutputCol(\"tokens\")\n",
    "\n",
    "#Defining the stopwords using stopwordsremover function\n",
    "extra_stopwords = ['http','amp','rt','t','c','the']\n",
    "stopwords_remover = StopWordsRemover()\\\n",
    "                    .setInputCol('tokens')\\\n",
    "                    .setOutputCol('filtered_words')\\\n",
    "                    .setStopWords(extra_stopwords)\n",
    "                    \n",
    "\n",
    "#Defining a set of words using countVectorizer function\n",
    "count_vectors = CountVectorizer(vocabSize=10000, minDF=5)\\\n",
    "               .setInputCol(\"filtered_words\")\\\n",
    "               .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "#Defining TF-IDF to vectorise features \n",
    "hashingTf = HashingTF(numFeatures=10000)\\\n",
    "            .setInputCol(\"filtered_words\")\\\n",
    "            .setOutputCol(\"raw_features\")\n",
    "            \n",
    "#Using minDocFreq to remove sparse terms\n",
    "idf = IDF(minDocFreq=5)\\\n",
    "        .setInputCol(\"raw_features\")\\\n",
    "        .setOutputCol(\"features\")\n",
    "\n",
    "#Encoding the Category variable into label using StringIndexer\n",
    "label_string_idx = StringIndexer()\\\n",
    "                  .setInputCol(\"Primary Type\")\\\n",
    "                  .setOutputCol(\"label\")\n",
    "\n",
    "#Defining classifier structure for logistic Regression imported from the library\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "#Defining classifier structure for Naive Bayes\n",
    "nb = NaiveBayes(smoothing=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using logistic regression by converting features into count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model using Logistic Regression after converting the features into vectors\n",
    "pipeline_cv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, lr])\n",
    "model_cv_lr = pipeline_cv_lr.fit(training)\n",
    "predictions_cv_lr = model_cv_lr.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the TOP Predictions\n",
      "+---------------------------+------------+------------------------------+-----+----------+\n",
      "|                Description|Primary Type|                   probability|label|prediction|\n",
      "+---------------------------+------------+------------------------------+-----+----------+\n",
      "|from coin-op machine/device|       theft|[0.8425360622808422,0.02801...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8425360622808422,0.02801...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8425360622808422,0.02801...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8425360622808422,0.02801...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8425360622808422,0.02801...|  0.0|       0.0|\n",
      "+---------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the top predictions of the model\n",
    "print('Displaying the TOP Predictions')\n",
    "predictions_cv_lr.select('Description','Primary Type',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model\n",
      "accuracy:89.0%\n"
     ]
    }
   ],
   "source": [
    "#Determing the accuracy of the model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "evaluator_cv_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_lr)\n",
    "print('Accuracy of the model')\n",
    "print('accuracy:{}%'.format(np.round(evaluator_cv_lr,2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Secondary model using NaiveBayes\n",
    "pipeline_cv_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors,label_string_idx, nb])\n",
    "model_cv_nb = pipeline_cv_nb.fit(training)\n",
    "predictions_cv_nb = model_cv_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model\n",
      "accuracy:92.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluting the accuracy of the model and displaying the accuracy\n",
    "evaluator_cv_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_cv_nb)\n",
    "print('Accuracy of the model')\n",
    "print('accuracy:{}%'.format(np.round(evaluator_cv_nb,2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using Logistic regression with the help of TF-IDF\n",
    "pipeline_idf_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, lr])\n",
    "model_idf_lr = pipeline_idf_lr.fit(training)\n",
    "predictions_idf_lr = model_idf_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few predictions of the model\n",
      " \n",
      "+---------------------------+------------+------------------------------+-----+----------+\n",
      "|                Description|Primary Type|                   probability|label|prediction|\n",
      "+---------------------------+------------+------------------------------+-----+----------+\n",
      "|from coin-op machine/device|       theft|[0.8424143819961089,0.02800...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8424143819961089,0.02800...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8424143819961089,0.02800...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8424143819961089,0.02800...|  0.0|       0.0|\n",
      "|from coin-op machine/device|       theft|[0.8424143819961089,0.02800...|  0.0|       0.0|\n",
      "+---------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the top predictions of the model\n",
    "print('Few predictions of the model')\n",
    "print(' ')\n",
    "predictions_idf_lr.select('Description','Primary Type',\"probability\",\"label\",\"prediction\")\\\n",
    "                                        .orderBy(\"probability\", ascending=False)\\\n",
    "                                        .show(n=5, truncate=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model\n",
      "accuracy:89.0%\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model and displaying the accuracy of the model using test data\n",
    "evaluator_idf_lr = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_lr)\n",
    "print('Accuracy of the model')\n",
    "print('accuracy:{}%'.format(np.round(evaluator_idf_lr,2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model using Naive Bayes algorithm using the TF-IDF function\n",
    "pipeline_idf_nb = Pipeline().setStages([regex_tokenizer,stopwords_remover,hashingTf, idf, label_string_idx, nb])\n",
    "model_idf_nb = pipeline_idf_nb.fit(training)\n",
    "predictions_idf_nb = model_idf_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model\n",
      "accuracy:92.0%\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model and displaying the accuracy of the model\n",
    "evaluator_idf_nb = MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").evaluate(predictions_idf_nb)\n",
    "print('Accuracy of the model')\n",
    "print('accuracy:{}%'.format(np.round(evaluator_idf_nb,2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
